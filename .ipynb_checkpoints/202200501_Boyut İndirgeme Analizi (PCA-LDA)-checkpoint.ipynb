{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "#                     BOYUT İNDİRGEME                   :\n",
    "------------------------------------------------------------------------\n",
    "1. BOYUT DÖNÜŞTÜRME GEREKSİZ BOYUTLARDAN KURTULMA VEYA BAZILARINI BİRLEŞTİRME \n",
    "2. DEĞİŞKENLER ARASINDAKİ BAĞLANTI (KORELASYONLARI ORTAYA ÇIKARMA)\n",
    "3. EIGEN VALUE : ÖZ DEĞER (SKALAR),\n",
    "4. EIGEN VECTOR : ÖZ YÖNEY (VEKTÖR)\n",
    "        \n",
    "+1 2 +0      1       3 (1*1 +2*1+0*1)           1\n",
    "+0 1 +2   *  1   =   3                  =    3  1\n",
    "-1 0 -2      0       0                          0\n",
    "\n",
    "\n",
    "BURADA ;\n",
    "\n",
    "ÇARPAN +3 ÖZ DEĞER,\n",
    "VEKTOR [1,1,0] ÖZ YÖNEY\n",
    "\n",
    "1. PCA (PRINCIPAL COMPANANT ANALYSIS- BİRİNCİL BİLEŞEN ANALİZİ) ALGORİTMASI    :\n",
    "-----------------------------------------------------------------------------\n",
    "UNSUPERVISED GÖZETİMSİZ YAPIDADIR. CLUSTERING VARDIR. SINIF ETİKETLERİ IGNORE EDİLİYOR.\n",
    "\n",
    "1. İNDİRGENECEK BOYUT : K OLSUN\n",
    "2. VERİ STANDARTLAŞTIRMASI YAP\n",
    "3. KOVARYANS VE KORELASYON MATRİSİNDEN ÖZ DEĞER VE ÖZ YÖNEY ELDE ET.\n",
    "4. ÖZ DEĞERLERİ AZALARAK SIRALA VE K TANESİNİ AL.\n",
    "5. K ÖZDEĞERDEN W PROJESİ MATRIX OLUŞTUR.\n",
    "6. ORJİNAL VERİ KUMESİ [X] ; W KULLANARAK DÖÜŞTÜR VE K-BOYUT UZAY ELDE ET.\n",
    "\n",
    "2. LDA (LINEAR DISCRIMINANT ANALYSİS- DOĞRUSAL AYRIŞMA ANALİZİ) ALGORİTMASI    :\n",
    "-----------------------------------------------------------------------------\n",
    "SUPERVISED GÖZETİMLİ YAPIDADIR. ETİKETLİ VERİLER OLUR. VERİLERİN SINIFLARINI AYRIŞTIRIR.\n",
    "sebastianraschka.com sayfasında detaylı LDA makalesi mevcuttur.\n",
    "\n",
    "VERİ SINIFLARININ AYRIŞTIRILMASI HARİÇ PCA İLE AYNIDIR. \n",
    "\n",
    "NEREDE KULLANILACAK                                                          :\n",
    "------------------------------------------------------------------------------\n",
    "1. GÜRÜLTÜ FİLTERELEME : ANOMALİYE KAŞI DERENÇ OLUŞTURUR.\n",
    "2. GÖRSELLEŞTİRME\n",
    "3. ÖZ NİTELİK ÇIKARIMI : FEATURE EXTRACTION\n",
    "4. ÖZ NİTELİK NİTELİK ELEME YADA DÖNÜŞTÜRME : GEREKSİZ FEATURE VARSA ELEME YAPILIR.\n",
    "5. BORSA ANALİZİ\n",
    "6. SAĞLIK VE GENETİK VERİLER \n",
    "7. YER BİLİMİ GEO SCIENCE VB...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. KUTUPHANELERİN YUKLENMESİ [2.1-2.2]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Alcohol  Malic_Acid   Ash  Ash_Alcanity  Magnesium  Total_Phenols  \\\n",
      "0      14.23        1.71  2.43          15.6        127           2.80   \n",
      "1      13.20        1.78  2.14          11.2        100           2.65   \n",
      "2      13.16        2.36  2.67          18.6        101           2.80   \n",
      "3      14.37        1.95  2.50          16.8        113           3.85   \n",
      "4      13.24        2.59  2.87          21.0        118           2.80   \n",
      "..       ...         ...   ...           ...        ...            ...   \n",
      "173    13.71        5.65  2.45          20.5         95           1.68   \n",
      "174    13.40        3.91  2.48          23.0        102           1.80   \n",
      "175    13.27        4.28  2.26          20.0        120           1.59   \n",
      "176    13.17        2.59  2.37          20.0        120           1.65   \n",
      "177    14.13        4.10  2.74          24.5         96           2.05   \n",
      "\n",
      "     Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity   Hue  \\\n",
      "0          3.06                  0.28             2.29             5.64  1.04   \n",
      "1          2.76                  0.26             1.28             4.38  1.05   \n",
      "2          3.24                  0.30             2.81             5.68  1.03   \n",
      "3          3.49                  0.24             2.18             7.80  0.86   \n",
      "4          2.69                  0.39             1.82             4.32  1.04   \n",
      "..          ...                   ...              ...              ...   ...   \n",
      "173        0.61                  0.52             1.06             7.70  0.64   \n",
      "174        0.75                  0.43             1.41             7.30  0.70   \n",
      "175        0.69                  0.43             1.35            10.20  0.59   \n",
      "176        0.68                  0.53             1.46             9.30  0.60   \n",
      "177        0.76                  0.56             1.35             9.20  0.61   \n",
      "\n",
      "     OD280  Proline  Customer_Segment  \n",
      "0     3.92     1065                 1  \n",
      "1     3.40     1050                 1  \n",
      "2     3.17     1185                 1  \n",
      "3     3.45     1480                 1  \n",
      "4     2.93      735                 1  \n",
      "..     ...      ...               ...  \n",
      "173   1.74      740                 3  \n",
      "174   1.56      750                 3  \n",
      "175   1.56      835                 3  \n",
      "176   1.62      840                 3  \n",
      "177   1.60      560                 3  \n",
      "\n",
      "[178 rows x 14 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Alcohol               178 non-null    float64\n",
      " 1   Malic_Acid            178 non-null    float64\n",
      " 2   Ash                   178 non-null    float64\n",
      " 3   Ash_Alcanity          178 non-null    float64\n",
      " 4   Magnesium             178 non-null    int64  \n",
      " 5   Total_Phenols         178 non-null    float64\n",
      " 6   Flavanoids            178 non-null    float64\n",
      " 7   Nonflavanoid_Phenols  178 non-null    float64\n",
      " 8   Proanthocyanins       178 non-null    float64\n",
      " 9   Color_Intensity       178 non-null    float64\n",
      " 10  Hue                   178 non-null    float64\n",
      " 11  OD280                 178 non-null    float64\n",
      " 12  Proline               178 non-null    int64  \n",
      " 13  Customer_Segment      178 non-null    int64  \n",
      "dtypes: float64(11), int64(3)\n",
      "memory usage: 19.6 KB\n",
      "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "## 2. VERİNİN (.CSV) İÇE YÜKLEMESİ ve PD DATAFRAME OLUŞTURMA (comma seperated value virgulle ayrılan veriler ) [2.3]-[2.4]\n",
    "\n",
    "veriler = pd.read_csv(\"20220225_PCA Boyut İndirgeme Analizi_PSA_Wine.csv\")\n",
    "# MUSTERİ KAYIP ANALİZİ [KAYBEDECEĞİMİZİ NASIL ANLARIZ]\n",
    "# YENİ MUSTERİ KAZANMAK ELDEKİ MUSTERİYİ KAYETMEMEKTEN 3 KAT DAHA PAHALI BUNDAN DOLAYI BU ANALİZ ÖNEMLİ\n",
    "print(veriler)\n",
    "veriler.info()\n",
    "'''\n",
    "178 rows x 14 columns]\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 178 entries, 0 to 177\n",
    "Data columns (total 14 columns):\n",
    " #   Column                Non-Null Count  Dtype  \n",
    "---  ------                --------------  -----  \n",
    " 0   Alcohol               178 non-null    float64 : Bağımsız Değişken\n",
    " 1   Malic_Acid            178 non-null    float64 : Bağımsız Değişken\n",
    " 2   Ash                   178 non-null    float64 : Bağımsız Değişken\n",
    " 3   Ash_Alcanity          178 non-null    float64 : Bağımsız Değişken\n",
    " 4   Magnesium             178 non-null    int64   : Bağımsız Değişken\n",
    " 5   Total_Phenols         178 non-null    float64 : Bağımsız Değişken\n",
    " 6   Flavanoids            178 non-null    float64 : Bağımsız Değişken\n",
    " 7   Nonflavanoid_Phenols  178 non-null    float64 : Bağımsız Değişken\n",
    " 8   Proanthocyanins       178 non-null    float64 : Bağımsız Değişken\n",
    " 9   Color_Intensity       178 non-null    float64 : Bağımsız Değişken\n",
    " 10  Hue                   178 non-null    float64 : Bağımsız Değişken\n",
    " 11  OD280                 178 non-null    float64 : Bağımsız Değişken\n",
    " 12  Proline               178 non-null    int64   : Bağımsız Değişken\n",
    " 13  Customer_Segment      178 non-null    int64   : BAĞIMLI DEĞİŞKEN Y\n",
    "\n",
    "dtypes: float64(11), int64(3)\n",
    "memory usage: 19.6 KB\n",
    "\n",
    "'''\n",
    "# 2a. ON İŞLEME \n",
    "  # 2a1. Pandas Dataframe içindeki sutunları X (BAGIMSIZ) VE Y(BAĞIMLI) DEĞİŞKEN DİZİLERİ OLARAK AYIRMA\n",
    "     # İlk 3 değişken gereksiz bu yüzden bağımsız x değişkenlerinden ayrılacak.\n",
    "      # Excited 0 Bırakmayanlar Y Bağımlı Değişkeni \n",
    "  \n",
    "X = veriler.iloc[:,0:13].values \n",
    "# [ tüm satırlar, 0-13 arasındaki tüm sütünlar] iloc ile X Bağımsız değişkenleri alındı.\n",
    "# Pandas Dataframeden .values ile numpy array yapıldı\n",
    "Y = veriler.iloc[:,13].values \n",
    "# [ tüm satırlar, 13 ncü sutun ] Y Bağımlı değişkeni customer segment alındı.\n",
    "# Pandas Dataframeden .values ile numpy array yapıldı\n",
    "\n",
    "print (X),\n",
    "print (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.369e+01 3.260e+00 2.540e+00 ... 9.600e-01 1.820e+00 6.800e+02]\n",
      " [1.269e+01 1.530e+00 2.260e+00 ... 9.600e-01 2.060e+00 4.950e+02]\n",
      " [1.162e+01 1.990e+00 2.280e+00 ... 1.160e+00 2.960e+00 3.450e+02]\n",
      " ...\n",
      " [1.242e+01 1.610e+00 2.190e+00 ... 1.060e+00 2.960e+00 3.450e+02]\n",
      " [1.390e+01 1.680e+00 2.120e+00 ... 9.100e-01 3.330e+00 9.850e+02]\n",
      " [1.416e+01 2.510e+00 2.480e+00 ... 6.200e-01 1.710e+00 6.600e+02]]\n",
      "[3 2 2 3 1 1 2 2 2 1 3 2 3 1 3 3 1 3 1 2 3 3 2 3 3 1 2 3 2 2 3 2 1 2 2 2 1\n",
      " 1 2 2 3 3 2 2 2 3 3 1 3 2 2 2 2 2 1 1 2 1 3 1 3 1 1 2 1 2 2 1 3 2 1 2 2 2\n",
      " 3 1 3 3 1 1 2 3 1 1 2 2 1 1 1 3 2 1 2 3 1 2 3 3 1 1 3 1 3 2 1 1 2 1 3 2 3\n",
      " 1 3 3 3 1 2 2 2 2 3 3 2 2 1 2 3 3 1 1 3 2 2 2 1 1 1 2 2 2 1 3]\n",
      "[[1.374e+01 1.670e+00 2.250e+00 1.640e+01 1.180e+02 2.600e+00 2.900e+00\n",
      "  2.100e-01 1.620e+00 5.850e+00 9.200e-01 3.200e+00 1.060e+03]\n",
      " [1.279e+01 2.670e+00 2.480e+00 2.200e+01 1.120e+02 1.480e+00 1.360e+00\n",
      "  2.400e-01 1.260e+00 1.080e+01 4.800e-01 1.470e+00 4.800e+02]\n",
      " [1.237e+01 1.130e+00 2.160e+00 1.900e+01 8.700e+01 3.500e+00 3.100e+00\n",
      "  1.900e-01 1.870e+00 4.450e+00 1.220e+00 2.870e+00 4.200e+02]\n",
      " [1.356e+01 1.730e+00 2.460e+00 2.050e+01 1.160e+02 2.960e+00 2.780e+00\n",
      "  2.000e-01 2.450e+00 6.250e+00 9.800e-01 3.030e+00 1.120e+03]\n",
      " [1.305e+01 5.800e+00 2.130e+00 2.150e+01 8.600e+01 2.620e+00 2.650e+00\n",
      "  3.000e-01 2.010e+00 2.600e+00 7.300e-01 3.100e+00 3.800e+02]\n",
      " [1.156e+01 2.050e+00 3.230e+00 2.850e+01 1.190e+02 3.180e+00 5.080e+00\n",
      "  4.700e-01 1.870e+00 6.000e+00 9.300e-01 3.690e+00 4.650e+02]\n",
      " [1.406e+01 2.150e+00 2.610e+00 1.760e+01 1.210e+02 2.600e+00 2.510e+00\n",
      "  3.100e-01 1.250e+00 5.050e+00 1.060e+00 3.580e+00 1.295e+03]\n",
      " [1.236e+01 3.830e+00 2.380e+00 2.100e+01 8.800e+01 2.300e+00 9.200e-01\n",
      "  5.000e-01 1.040e+00 7.650e+00 5.600e-01 1.580e+00 5.200e+02]\n",
      " [1.225e+01 1.730e+00 2.120e+00 1.900e+01 8.000e+01 1.650e+00 2.030e+00\n",
      "  3.700e-01 1.630e+00 3.400e+00 1.000e+00 3.170e+00 5.100e+02]\n",
      " [1.208e+01 1.830e+00 2.320e+00 1.850e+01 8.100e+01 1.600e+00 1.500e+00\n",
      "  5.200e-01 1.640e+00 2.400e+00 1.080e+00 2.270e+00 4.800e+02]\n",
      " [1.336e+01 2.560e+00 2.350e+00 2.000e+01 8.900e+01 1.400e+00 5.000e-01\n",
      "  3.700e-01 6.400e-01 5.600e+00 7.000e-01 2.470e+00 7.800e+02]\n",
      " [1.388e+01 5.040e+00 2.230e+00 2.000e+01 8.000e+01 9.800e-01 3.400e-01\n",
      "  4.000e-01 6.800e-01 4.900e+00 5.800e-01 1.330e+00 4.150e+02]\n",
      " [1.420e+01 1.760e+00 2.450e+00 1.520e+01 1.120e+02 3.270e+00 3.390e+00\n",
      "  3.400e-01 1.970e+00 6.750e+00 1.050e+00 2.850e+00 1.450e+03]\n",
      " [1.237e+01 1.070e+00 2.100e+00 1.850e+01 8.800e+01 3.520e+00 3.750e+00\n",
      "  2.400e-01 1.950e+00 4.500e+00 1.040e+00 2.770e+00 6.600e+02]\n",
      " [1.358e+01 2.580e+00 2.690e+00 2.450e+01 1.050e+02 1.550e+00 8.400e-01\n",
      "  3.900e-01 1.540e+00 8.660e+00 7.400e-01 1.800e+00 7.500e+02]\n",
      " [1.200e+01 9.200e-01 2.000e+00 1.900e+01 8.600e+01 2.420e+00 2.260e+00\n",
      "  3.000e-01 1.430e+00 2.500e+00 1.380e+00 3.120e+00 2.780e+02]\n",
      " [1.376e+01 1.530e+00 2.700e+00 1.950e+01 1.320e+02 2.950e+00 2.740e+00\n",
      "  5.000e-01 1.350e+00 5.400e+00 1.250e+00 3.000e+00 1.235e+03]\n",
      " [1.419e+01 1.590e+00 2.480e+00 1.650e+01 1.080e+02 3.300e+00 3.930e+00\n",
      "  3.200e-01 1.860e+00 8.700e+00 1.230e+00 2.820e+00 1.680e+03]\n",
      " [1.264e+01 1.360e+00 2.020e+00 1.680e+01 1.000e+02 2.020e+00 1.410e+00\n",
      "  5.300e-01 6.200e-01 5.750e+00 9.800e-01 1.590e+00 4.500e+02]\n",
      " [1.383e+01 1.650e+00 2.600e+00 1.720e+01 9.400e+01 2.450e+00 2.990e+00\n",
      "  2.200e-01 2.290e+00 5.600e+00 1.240e+00 3.370e+00 1.265e+03]\n",
      " [1.311e+01 1.010e+00 1.700e+00 1.500e+01 7.800e+01 2.980e+00 3.180e+00\n",
      "  2.600e-01 2.280e+00 5.300e+00 1.120e+00 3.180e+00 5.020e+02]\n",
      " [1.305e+01 1.650e+00 2.550e+00 1.800e+01 9.800e+01 2.450e+00 2.430e+00\n",
      "  2.900e-01 1.440e+00 4.250e+00 1.120e+00 2.510e+00 1.105e+03]\n",
      " [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n",
      "  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]\n",
      " [1.251e+01 1.730e+00 1.980e+00 2.050e+01 8.500e+01 2.200e+00 1.920e+00\n",
      "  3.200e-01 1.480e+00 2.940e+00 1.040e+00 3.570e+00 6.720e+02]\n",
      " [1.233e+01 1.100e+00 2.280e+00 1.600e+01 1.010e+02 2.050e+00 1.090e+00\n",
      "  6.300e-01 4.100e-01 3.270e+00 1.250e+00 1.670e+00 6.800e+02]\n",
      " [1.252e+01 2.430e+00 2.170e+00 2.100e+01 8.800e+01 2.550e+00 2.270e+00\n",
      "  2.600e-01 1.220e+00 2.000e+00 9.000e-01 2.780e+00 3.250e+02]\n",
      " [1.243e+01 1.530e+00 2.290e+00 2.150e+01 8.600e+01 2.740e+00 3.150e+00\n",
      "  3.900e-01 1.770e+00 3.940e+00 6.900e-01 2.840e+00 3.520e+02]\n",
      " [1.216e+01 1.610e+00 2.310e+00 2.280e+01 9.000e+01 1.780e+00 1.690e+00\n",
      "  4.300e-01 1.560e+00 2.450e+00 1.330e+00 2.260e+00 4.950e+02]\n",
      " [1.176e+01 2.680e+00 2.920e+00 2.000e+01 1.030e+02 1.750e+00 2.030e+00\n",
      "  6.000e-01 1.050e+00 3.800e+00 1.230e+00 2.500e+00 6.070e+02]\n",
      " [1.378e+01 2.760e+00 2.300e+00 2.200e+01 9.000e+01 1.350e+00 6.800e-01\n",
      "  4.100e-01 1.030e+00 9.580e+00 7.000e-01 1.680e+00 6.150e+02]\n",
      " [1.339e+01 1.770e+00 2.620e+00 1.610e+01 9.300e+01 2.850e+00 2.940e+00\n",
      "  3.400e-01 1.450e+00 4.800e+00 9.200e-01 3.220e+00 1.195e+03]\n",
      " [1.422e+01 1.700e+00 2.300e+00 1.630e+01 1.180e+02 3.200e+00 3.000e+00\n",
      "  2.600e-01 2.030e+00 6.380e+00 9.400e-01 3.310e+00 9.700e+02]\n",
      " [1.204e+01 4.300e+00 2.380e+00 2.200e+01 8.000e+01 2.100e+00 1.750e+00\n",
      "  4.200e-01 1.350e+00 2.600e+00 7.900e-01 2.570e+00 5.800e+02]\n",
      " [1.421e+01 4.040e+00 2.440e+00 1.890e+01 1.110e+02 2.850e+00 2.650e+00\n",
      "  3.000e-01 1.250e+00 5.240e+00 8.700e-01 3.330e+00 1.080e+03]\n",
      " [1.483e+01 1.640e+00 2.170e+00 1.400e+01 9.700e+01 2.800e+00 2.980e+00\n",
      "  2.900e-01 1.980e+00 5.200e+00 1.080e+00 2.850e+00 1.045e+03]\n",
      " [1.305e+01 1.770e+00 2.100e+00 1.700e+01 1.070e+02 3.000e+00 3.000e+00\n",
      "  2.800e-01 2.030e+00 5.040e+00 8.800e-01 3.350e+00 8.850e+02]]\n",
      "[1 3 2 1 2 2 1 3 2 2 3 3 1 2 3 2 1 1 2 1 2 1 1 2 2 2 2 2 2 3 1 1 2 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# 3. VERİLERİN TEST VE TRAIN OLARAK BÖLÜNMESİ [2.8]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=0)\n",
    "# Dataset 20% test ve 80%train olarak ikiye ile 13 X bağımsız ve 1 Y bağımlı değişken olarak 4 e ayrıldı.\n",
    "\n",
    "print(x_train) # train 13 bağımsız değişkenler % 80 Dataset\n",
    "print(y_train) # train 1 bağımlı değişken      % 80 Dataset\n",
    "print(x_test)  # test 13 bağımsız değişken     % 20 Dataset\n",
    "print(y_test)  # train 1 bağımlı değişken      % 20 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. VERİLERİN ÖZNİTELİK SCALER ÖLÇEKLENMESİ [2.9] ANN-DEEP LEARNING İÇİN DEĞERLER 0-1 ARASI OLACAK\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(x_train) # X_train scale edilmiş bağımsız değişkenler train datalar \n",
    "X_test = sc.fit_transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. PCA VE LDA İLE BOYUT AZALTMA İŞLEMLERİ:\n",
    "          \n",
    "# 21a.PCA MODEL OLUŞTURMA VE BOYUT DÖNÜŞTÜRME: \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2) # 2 boyutlu PCA obje urettik.\n",
    "\n",
    "# 21b.LDA MODEL OLUŞTURMA VE BOYUT DÖNÜŞTÜRME: \n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components = 2) # 2 boyutlu LDA obje urettik.\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# 13 adet X Bağımsız X_train eğitim ve X_test test değişkenlerine PCA uygulanması\n",
    "# Burada önemli olan pca objesinin fit edilmesi. Sadece train set ile eğtiliyor. Eğitilen pca ile train ve test setler transform ediliyor.\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train) # Fit the model with X_train and apply the dimensionality reduction on X_train_pca.\n",
    "# Ölçeklendirilmiş X_train eğitim verisi (toplam verinin 80%) ile fit eğitilip veri yeni hal alıcak.\n",
    "# Bu süreçte 13 bağımsız değişken 2 boyuta dönüşecek. Kayıpalr olabilir.\n",
    "\n",
    "X_train_lda = lda.fit_transform(X_train,y_train) \n",
    "# DİKKAT : neden scale edilmemiş y_trainde fit_transform ediliyor?.\n",
    "# LDA SUPERVİSED gözetimli yani etiketleri [y_train] bağımlı değişkenleride kullanacaktır. LDA-PCA arasındaki TEK FARK BU.... \n",
    "# Etiketler sayesinde sınıflar araındaki farkları maksimize ediyor.\n",
    "\n",
    "X_test_pca = pca.transform(X_test) # Apply dimensionality reduction to X.\n",
    "# Ölçeklendirilmiş X_test verisi; X_Train ile eğitilen (fit edilen) pca objesi ile transform edilerek 13 (X) değişkenden 2 (X) Değişkene boyut azalttı.\n",
    "# BURADA DİKKAT TEST İÇİN FIT YOK .. daha önce it edilen X_train pca objesi ile eğitilior. \n",
    "# TRAINDE FİT EDİLİP EĞİTİLEN MODEL TESTTE TRANSFORM EDİLİYOR. \n",
    "# TEKRAR TEST KUMESİ İLE TRAIN YAPILMIYIR.\n",
    "\n",
    "X_test_lda = lda.transform(X_test) # lda objesi trainde fit edildiği için sadece X_test verilerini yazıyoruz.\n",
    "# Yani ölçekli X_train ve ölçeksik y_train ile fit edilen lda objesi ile _test verileri transform ediliyor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 21a. SCALE - FIT VE TRANSFORM EDİLEN VERİLERİN SINIFLANDIRILMASI (LOJİSTİK REGRESYONLA YAPALIM) İŞLEMLERİ: \n",
    "     # 5. LOJISTIK REGRESYON VE VİZUALİZATION [GÖRSELLEŞTİRME] :\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression # LR ile sınıflandırma yapacağız.\n",
    "      \n",
    "    # PCA boyut azaltma yapılmadan yapılan Sınıflandırma işlemi:\n",
    "logr = LogisticRegression(random_state=0) # random state 0 ile aşağıdaki ve bu sınıflandırada aynı algoritma kullanılacak.\n",
    "logr.fit(X_train,y_train) \n",
    "# Dikkat Boyut azaltma olmayan X_train ile fit edip eğitim setindeki y değişkeni sınıflandırılıyor\n",
    "# Sınıflandırma yapılan set logr.fit objesinde,\n",
    "\n",
    "    # PCA boyut azaltma yapıldıktan sonra yapılan Sınıflandırma işlemi:\n",
    "logr_pca = LogisticRegression(random_state=0)\n",
    "logr_pca.fit(X_train_pca,y_train) # Dikkat Boyut azaltma yapılan X_train_pca ile fit edip eğitim setindeki y değişkeni sınıflandırılıyor\n",
    "\n",
    "   # LDA boyut azaltma yapıldıktan sonra yapılan Sınıflandırma işlemi:\n",
    "logr_lda = LogisticRegression(random_state=0)\n",
    "logr_lda.fit(X_train_lda,y_train) # Ölçeklenmiş ve 2 Boyuta LDA ile inmiş X_train eğitim verilerinden y_train verilerini öğrenecek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.e PREDICT TAHMİN ETMEK :\n",
    "'''\n",
    "Model.predict(\n",
    "    x,\n",
    "    batch_size=None,\n",
    "    verbose=0,\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False,\n",
    ")\n",
    "'''\n",
    "y_pred = logr.predict(X_test) \n",
    "# PSA YAPILMADAN 13 BAĞIMSIZ DEĞİŞKEN (X_test) İLE YAPILAN BAĞIMLI DEĞİŞKEN TAHMİNİ \n",
    "y_pred_psa = logr_pca.predict(X_test_pca) \n",
    "# PSA YAPILDIKTAN SONRA 2 BAĞIMSIZ DEĞİŞKEN (X_test_psa) İLE YAPILAN BAĞIMLI DEĞİKEN TAHMİNİ \n",
    "y_pred_lda = logr_lda.predict(X_test_lda) \n",
    "# LDA YAPILDIKTAN SONRA 2 BAĞIMSIZ DEĞİŞKEN (X_test_lda) İLE YAPILAN BAĞIMLI DEĞİKEN TAHMİNİ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Y_test (GERÇEK SONUÇ) İLE Y_Pred (PSA olmadan tahmin Karşılaştırması) :\n",
      "[[14  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  0  6]]\n",
      " Y_test (GERÇEK SONUÇ) İLE Y_Pred_psa (PSA yapılarak tahmin Karşılaştırması) :\n",
      "[[14  0  0]\n",
      " [ 1 15  0]\n",
      " [ 0  0  6]]\n",
      " Y_Pred (Tahmni Sonuç) İLE Y_Pred_psa (PSA yapılarak tahmin Karşılaştırması) :\n",
      "[[14  0  0]\n",
      " [ 1 14  0]\n",
      " [ 0  1  6]]\n",
      " Y_test (GERÇEK SONUÇ) İLE Y_Pred_lda (LDA yapılarak tahmin Karşılaştırması) :\n",
      "[[14  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  0  6]]\n"
     ]
    }
   ],
   "source": [
    "# 12. KARŞILAŞTIRMA MATRIXI\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\" Y_test (GERÇEK SONUÇ) İLE Y_Pred (PSA olmadan tahmin Karşılaştırması) :\")\n",
    "cm_1 = confusion_matrix(y_test,y_pred)\n",
    "print(cm_1)\n",
    "\n",
    "\n",
    "print(\" Y_test (GERÇEK SONUÇ) İLE Y_Pred_psa (PSA yapılarak tahmin Karşılaştırması) :\")\n",
    "cm_2 = confusion_matrix(y_test,y_pred_psa)\n",
    "print(cm_2)\n",
    "\n",
    "print(\" Y_Pred (Tahmni Sonuç) İLE Y_Pred_psa (PSA yapılarak tahmin Karşılaştırması) :\")\n",
    "cm_3 = confusion_matrix(y_pred,y_pred_psa)\n",
    "print(cm_3)\n",
    "\n",
    "print(\" Y_test (GERÇEK SONUÇ) İLE Y_Pred_lda (LDA yapılarak tahmin Karşılaştırması) :\")\n",
    "cm_4 = confusion_matrix(y_test,y_pred_lda)\n",
    "print(cm_4) # LDA - Gerçek sonuç mukemmel çıktı % 100 başarılı oldu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. CONFUSION MATRIX (KARMAŞIKLIK MATRISI) :\n",
    "\n",
    "#  G                TAHMİN PREDICTION\n",
    "#  E -----------------------------------------------\n",
    "#  R   True-Positive  (TP)   +   False-Negative (FN)\n",
    "#  Ç   +++++++++++++++++++++++++++++++++++++++++++++\n",
    "#  E   False-Positive (FP)   +   True-Negative  (TN)\n",
    "#  K\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred) # y_test gerçek ve y_pred tahmini cinsiyet karşılaştrması\n",
    "\n",
    "# Matris değerlerinin incelenmesi:\n",
    "Satır0 = cm[0] # TP+FN\n",
    "TP = Satır0[0]\n",
    "FN = Satır0[1]\n",
    "Satır1 = cm[1] # FP+TN\n",
    "FP = Satır1[0]\n",
    "TN = Satır1[1]\n",
    "\n",
    "\n",
    "print(cm)\n",
    "print (y_test,y_pred) # Hiçbiri tutmamış ..😎😯\n",
    "\n",
    "print(\"Accuracy Rate    [Doğruluk Oranı]= \",\"%\", ((TP+TN)/(TP+FN+FP+TN))*100)\n",
    "print(\"Error Rate       [Hata Oranı]=     \",\"%\", ((100-((TP+TN)/(TP+FN+FP+TN))*100)))\n",
    "print(\"Precision Rate   [Kesinlik Oranı]= \",\"%\", ((TP)/(TP+FP))*100)\n",
    "print(\"Specificity-True/Neg rate (TNR)=   \",\"%\", ((TN/(FP+TN))*100))\n",
    "print(\"Sensitivity-Recall Rate -True/Pos Rate [Hassasiyet Oranı]=\",\"%\",((TP)/(TP+FN))*100)\n",
    "# Burada en önemli nokta tahmin edilen bağımlı değişkeninin balanced olması gerkiyor. Yani %90 Erkek ve\n",
    "# 10% kadın olan veri setinde Accuracy Rate in 90% olması başarı değildir. Kadını bulamama ihtimallerine\n",
    "# rağmen 90% erkek bulacağı için Accuracy Rate hata yaptırır. Bundan dolayı diğer değerlerede bakılmalıdır."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
